task: none
type: auto

resume: False
resume_dir: './result'

accel: False # PyTorch 2.0 compile

seed: 666

distributed:
  type: none
  debug: False
  local_rank:
  global_rank:
  local_size: 1
  world_size: 1
  master_address: 'localhost'
  maste_port: 8888
  env_set: 
  env_copy:
  
hyperparameters:
  epoch: 1
  steps: 0
  batch_size: 0
  seed: 666